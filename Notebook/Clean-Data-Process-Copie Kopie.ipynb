{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# read excel dataset as csv because system does not allow in any other way \n",
    "csv_file_path = '../Data/Telco_customer_churn.xlsx'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    print(\"File read successfully as a CSV file.\")\n",
    "    \n",
    "    # Check the shape of your data\n",
    "    print(\"Shape of the data:\", data.shape)\n",
    "    \n",
    "    # Display the first few rows of the data\n",
    "    print(data.head(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {csv_file_path} was not found. Please check the file path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and basic info\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})\n",
    "\n",
    "\n",
    "# Distribution of the target variable\n",
    "sns.countplot(x='Churn Label', data=data)\n",
    "plt.title('Distribution of Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the pairplot to focus on key features\n",
    "key_features = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'Churn Value']\n",
    "sns.pairplot(data[key_features + ['Churn Label']], hue='Churn Label', diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preparation\n",
    "data['Total Charges'] = pd.to_numeric(data['Total Charges'], errors='coerce')\n",
    "data['Total Charges'] = data['Total Charges'].fillna(data['Total Charges'].median())\n",
    "\n",
    "# Normalize numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Monthly Charges', 'Total Charges']\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})\n",
    "\n",
    "# Correct the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Monthly Charges', y='Total Charges', hue='Churn Label', data=data)\n",
    "plt.title('Monthly Charges vs. Total Charges with Churn Highlighted')\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Total Charges')\n",
    "plt.legend(title='Churn Status', loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot description for Tenure Months Distribution for Churn and Non-Churn Customers\n",
    "\n",
    "\n",
    "This plot shows the distribution of the tenure (in months) of customers who churned (left the service) and those who did not.\n",
    " It helps to visualize the relationship between how long a customer has been with the service and their likelihood of leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to highlight relationship between Tenure and Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn Label', y='Tenure Months', data=data)\n",
    "plt.title('Tenure Months Distribution for Churn and Non-Churn Customers')\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Tenure Months')\n",
    "plt.annotate('Low Tenure, High Churn', xy=(1, 10), xytext=(1.5, 20),\n",
    "             arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "plt.show()\n",
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot describtion for relationship between Tenure and Churn\n",
    "\n",
    "\n",
    "\n",
    "This plot shows how long customers have been with the company (tenure months) and whether they have canceled (churn label).\n",
    "\n",
    " The boxes show the distribution of the length of stay for both groups: Customers who have churned (1), and customers who have not churned (0). \n",
    " \n",
    " The note \"Low tenure, high churn\" indicates that customers who have been with the company for a shorter period of time are more likely to quit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot description for Contract Type vs. Churn\n",
    "This plot compares the contract types (Month-to-month, One year, Two year) and their respective churn rates. It helps to identify which contract types are more prone to customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})\n",
    "\n",
    "# Plot to highlight impact of Contract Type on Churn using blue tones\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Contract', hue='Churn Label', data=data, palette='Blues')\n",
    "plt.title('Contract Type vs. Churn', fontsize=16, weight='bold')\n",
    "plt.xlabel('Contract Type', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Adjust annotation to match the reference image\n",
    "plt.annotate('Month-to-month has higher churn', xy=(0, 2100), xytext=(0.5, 2500),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05), fontsize=12)\n",
    "\n",
    "# Adjust the legend to match the reference image\n",
    "plt.legend(title='Churn Status', loc='upper right')\n",
    "\n",
    "# Ensure that the x-axis labels are correctly displayed\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot description for Monthly Charges vs. Total Charges with Churn Highlighted\n",
    "\n",
    "This scatter plot shows the relationship between monthly charges and total charges, with churn status highlighted.\n",
    "\n",
    " It helps to see if there is a pattern in the charges that correlates with customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Monthly Charges', 'Total Charges']\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})\n",
    "\n",
    "# Correct the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Monthly Charges', y='Total Charges', hue='Churn Label', data=data)\n",
    "plt.title('Monthly Charges vs. Total Charges with Churn Highlighted')\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Total Charges')\n",
    "plt.legend(title='Churn Status', loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plot to highlight impact of Contract Type on Churn\n",
    "\n",
    "This plot shows how the type of contract influences the churn rate. \n",
    "Customers with month-to-month contracts have a higher churn rate compared to customers with longer contract terms (one-year or two-year contracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'TotalCharges_per_Month']\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Convert Churn Label to descriptive labels\n",
    "data['Churn Label'] = data['Churn Label'].replace({0: 'No Churn', 1: 'Churn'})\n",
    "\n",
    "# Plot 1: Monthly Charges vs. Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn Label', y='Monthly Charges', data=data, palette=\"Set2\")\n",
    "plt.title('Monthly Charges Distribution for Churn and Non-Churn Customers')\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1 Conclusion:\n",
    "\n",
    "***You can see that customers who canceled the service had slightly higher monthly fees on average.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Total Charges vs. Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn Label', y='Total Charges', data=data, palette=\"Set1\")\n",
    "plt.title('Total Charges Distribution for Churn and Non-Churn Customers')\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Total Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2 Conclusion:\n",
    "\n",
    "***Customers who canceled the service paid less overall because they were often customers for a shorter period of time.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Tenure vs. Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn Label', y='Tenure Months', data=data, palette=\"Set3\")\n",
    "plt.title('Tenure Distribution for Churn and Non-Churn Customers')\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Tenure Months')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3 Conclusion:\n",
    "\n",
    "***Customers who canceled the service often used the service for a shorter period of time than customers who stayed.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preparation\n",
    "data['Total Charges'] = data['Total Charges'].replace(\" \", np.nan)\n",
    "data['Total Charges'] = pd.to_numeric(data['Total Charges'], errors='coerce')\n",
    "data['Total Charges'] = data['Total Charges'].fillna(data['Total Charges'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeroes with NaN to prevent division by zero\n",
    "data['Tenure Months'].replace(0, np.nan, inplace=True)\n",
    "data['TotalCharges_per_Month'] = data['Total Charges'] / data['Tenure Months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute remaining missing values\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = data.select_dtypes(include=[object]).columns.tolist()\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "data[numerical_columns] = imputer_num.fit_transform(data[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    if column not in ['CustomerID', 'Churn Reason']:\n",
    "        data[column] = le.fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# Prevent division by zero by replacing zeroes with NaN in 'Tenure Months'\n",
    "data['Tenure Months'].replace(0, np.nan, inplace=True)\n",
    "data['TotalCharges_per_Month'] = data['Total Charges'] / data['Tenure Months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'TotalCharges_per_Month']\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Ensure no NaN values remain in the data\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance before oversampling\n",
    "sns.countplot(x='Churn Value', data=data, palette=\"viridis\")\n",
    "plt.title('Class Distribution Before Oversampling')\n",
    "plt.xlabel('Churn Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Display the counts\n",
    "print(data['Churn Value'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = data.drop(['Churn Label', 'Churn Value'], axis=1)\n",
    "y = data['Churn Value']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample  # Importing resample for oversampling and undersampling\n",
    "\n",
    "\n",
    "# Oversampling the minority class\n",
    "train = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "train['Churn Value'] = y_train.values\n",
    "\n",
    "# Separate minority and majority classes\n",
    "churn = train[train['Churn Value'] == 1]\n",
    "no_churn = train[train['Churn Value'] == 0]\n",
    "\n",
    "# Upsample minority class\n",
    "churn_upsampled = resample(churn, replace=True, n_samples=len(no_churn), random_state=42)\n",
    "train_upsampled = pd.concat([churn_upsampled, no_churn])\n",
    "\n",
    "# Check the new class distribution after oversampling\n",
    "sns.countplot(x='Churn Value', data=train_upsampled, palette=\"cubehelix\")\n",
    "plt.title('Class Distribution after Oversampling')\n",
    "plt.xlabel('Churn Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Prepare the data for modeling\n",
    "X_train_upsampled = train_upsampled.drop(columns='Churn Value')\n",
    "y_train_upsampled = train_upsampled['Churn Value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution after SMOTE\n",
    "sns.countplot(x=y_train_smote, palette=\"magma\")\n",
    "plt.title('Class Distribution after SMOTE')\n",
    "plt.xlabel('Churn Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with oversampled data\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Evaluation with Oversampled Data:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for undersampled and SMOTE datasets\n",
    "log_reg.fit(X_train_downsampled, y_train_downsampled)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Evaluation with Undersampled Data:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "log_reg.fit(X_train_smote, y_train_smote)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Evaluation with SMOTE Data:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preparing Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = data.drop(['Churn Label', 'Churn Value'], axis=1)\n",
    "y = data['Churn Value']\n",
    "\n",
    "#Check for any remaining NaN values in X and y\n",
    "print(\"NaN values in X: \", X.isnull().sum().sum())\n",
    "print(\"NaN values in y: \", y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in X_train and X_test\n",
    "X_train = imputer_num.fit_transform(X_train)\n",
    "X_test = imputer_num.transform(X_test)\n",
    "\n",
    "# Ensure no NaN values remain in the imputed data\n",
    "print(\"NaN values in X_train after imputation: \", np.isnan(X_train).sum())\n",
    "print(\"NaN values in X_test after imputation: \", np.isnan(X_test).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Development and Initial Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model with RandomForest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Modeling and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the input data does not contain NaN values before fitting\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The output you are seeing comes from a GridSearchCV run, which is used to find the best hyperparameters for a model. Let's break down the different parts of this output:\n",
    "\n",
    "Fitting 3 folds for each of 12 candidates, totalling 36 fits:\n",
    "\n",
    "***GridSearchCV is performing cross-validation by splitting the data into 3 folds.***\n",
    "For each set of hyperparameters, the model is trained and evaluated three times (once for each fold).\n",
    "There are 12 combinations of hyperparameters (n_estimators: 2 values, learning_rate: 2 values, max_depth: 3 values). Therefore, a total of 36 trainings and evaluations are performed (12 combinations * 3 folds).\n",
    "[CV] END ...:\n",
    "\n",
    "***These lines show*** the results of individual training runs with specific hyperparameter combinations.\n",
    "For example, [CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time= 3.4s means that a model with learning_rate=0.1, max_depth=3, and n_estimators=100 was trained, and the training took 3.4 seconds.\n",
    "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(random_state=42), n_jobs=-1, param_grid={'learning_rate': [0.1, 0.01], 'max_depth': [3, 4, 5], 'n_estimators': [100, 200]}, verbose=2):\n",
    "\n",
    "***This is a summary of the GridSearchCV settings.***\n",
    "cv=3: Cross-validation with 3 folds.\n",
    "estimator=GradientBoostingClassifier(random_state=42): The model being optimized is a Gradient Boosting Classifier.\n",
    "n_jobs=-1: Use all available CPUs for training.\n",
    "param_grid: The hyperparameter combinations being tried.\n",
    "verbose=2: Detailed output during training.\n",
    "best_estimator_: GradientBoostingClassifier:\n",
    "\n",
    "GridSearchCV has found the best hyperparameters, and the best model is a GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here is an example of how you can further use the results from the GridSearchCV:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters and the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best Model Performance\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
